{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8f3dd59",
   "metadata": {},
   "source": [
    "# **Assignment - 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4fb6c7",
   "metadata": {},
   "source": [
    "Q1: K-Fold Cross Validation for Multiple Linear Regression (Least Square Error Fit)  \n",
    "Download the dataset regarding USA House Price Prediction from the following link:  \n",
    "https://drive.google.com/file/d/1O_NwpJT-8xGfU_-3llUl2sgPu0xllOrX/view?usp=sharing  \n",
    "Load the dataset and Implement 5- fold cross validation for multiple linear regression  \n",
    "(using least square error fit).  \n",
    "Steps:  \n",
    "  \n",
    "a) Divide the dataset into input features (all columns except price) and output variable\n",
    "(price) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f77e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "df = pd.read_csv(\"Data/USA_Housing.csv\")\n",
    "\n",
    "X = df.drop(\"Price\", axis=1).values\n",
    "y = df[\"Price\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37500de6",
   "metadata": {},
   "source": [
    "b) Scale the values of input features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af292d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8246f4f",
   "metadata": {},
   "source": [
    "c) Divide input and output features into five folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f778261",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X)\n",
    "indices = np.arange(n)\n",
    "np.random.shuffle(indices)  \n",
    "folds = np.array_split(indices, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af0add",
   "metadata": {},
   "source": [
    "d) Run five iterations, in each iteration consider one-fold as test set and remaining four sets as training set.   \n",
    "Find the beta (ð›½) matrix, predicted values, and R2_score\n",
    "for each iteration using least square error fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7a2849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "\n",
      "Beta (coefficients):\n",
      "[1232183.14157037  230860.82977106  163813.31169062  123159.24698459\n",
      "    1862.78381785  150779.51690422]\n",
      "\n",
      "First 5 predictions: [1308929.94118012 1791050.82771068 1757636.94449517 1247747.29551711\n",
      "  908104.4682454 ]\n",
      "\n",
      "First 5 actuals:     [1499153.081 1833208.579 1741052.96  1153135.22   999244.635]\n",
      "RÂ² = 0.9203, MSE = 10073301346.7909\n",
      "\n",
      "Fold 2\n",
      "\n",
      "Beta (coefficients):\n",
      "[1231704.18118797  229631.56013204  162896.37725785  120237.85792513\n",
      "    2636.42063697  150492.62173255]\n",
      "\n",
      "First 5 predictions: [ 975567.0208538  1010998.88461966  788964.54134108  686683.19769761\n",
      " 2057672.48086813]\n",
      "\n",
      "First 5 actuals:     [ 980161.6274  923246.8621  734562.1045  482689.7034 2219724.162 ]\n",
      "RÂ² = 0.9176, MSE = 10326721192.2157\n",
      "\n",
      "Fold 3\n",
      "\n",
      "Beta (coefficients):\n",
      "[1233195.94576496  230422.33489483  164251.10231764  121489.18266983\n",
      "    1243.82302154  150714.30346787]\n",
      "\n",
      "First 5 predictions: [1279653.62019732 1454418.16407668 2036296.96870113 1022160.98837747\n",
      "  886440.91620051]\n",
      "\n",
      "First 5 actuals:     [1153516.919  1434323.825  1976170.089   877247.2454  940138.9614]\n",
      "RÂ² = 0.9186, MSE = 10176794456.3367\n",
      "\n",
      "Fold 4\n",
      "\n",
      "Beta (coefficients):\n",
      "[1230477.44356092  229615.68012117  164418.59491643  120627.65264568\n",
      "    2847.03191725  152171.40763459]\n",
      "\n",
      "First 5 predictions: [1732735.43651338 1330136.42869102  659194.99422463 1587330.11293881\n",
      " 1052529.34114545]\n",
      "\n",
      "First 5 actuals:     [1598615.974  1267150.837   771002.0763 1632943.349   989903.6448]\n",
      "RÂ² = 0.9159, MSE = 10112005622.2567\n",
      "\n",
      "Fold 5\n",
      "\n",
      "Beta (coefficients):\n",
      "[1232804.34958133  229240.89014998  165693.91482907  121252.08094175\n",
      "    1616.82245148  150168.29181268]\n",
      "\n",
      "First 5 predictions: [1129576.45427597 1003481.35070444  439325.20307118 1824781.56740769\n",
      " 1849568.88097633]\n",
      "\n",
      "First 5 actuals:     [1029439.234  1057347.166   395440.2022 1880127.062  1848511.108 ]\n",
      "RÂ² = 0.9160, MSE = 10571877585.4418\n"
     ]
    }
   ],
   "source": [
    "def add_intercept(X):\n",
    "    return np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "def least_squares_beta(X, y):\n",
    "    # Normal equation: Î² = (X^T X)^(-1) X^T y\n",
    "    return np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "\n",
    "best_beta = None\n",
    "best_r2 = -np.inf\n",
    "\n",
    "for i in range(5):\n",
    "    test_idx = folds[i]\n",
    "    train_idx = np.hstack([folds[j] for j in range(5) if j != i])\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    X_train_int = add_intercept(X_train)\n",
    "    X_test_int = add_intercept(X_test)\n",
    "\n",
    "    beta = least_squares_beta(X_train_int, y_train)\n",
    "\n",
    "    y_pred = X_test_int @ beta\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nFold {i+1}\")\n",
    "    print(\"\\nBeta (coefficients):\")\n",
    "    print(beta) \n",
    "    print(\"\\nFirst 5 predictions:\", y_pred[:5])\n",
    "    print(\"\\nFirst 5 actuals:    \", y_test[:5])\n",
    "    print(f\"RÂ² = {r2:.4f}, MSE = {mse:.4f}\")\n",
    "\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_beta = beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110fa2d",
   "metadata": {},
   "source": [
    "e) Use the best value of (ð›½) matrix (for which R2_score is maximum), to train the\n",
    "regressor for 70% of data and test the performance for remaining 30% data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35091c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using best Î² on 70/30 split:\n",
      "RÂ² = 0.9147509985464375\n",
      "MSE = 10060262294.327486\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_int = add_intercept(X_train)\n",
    "X_test_int = add_intercept(X_test)\n",
    "\n",
    "y_pred = X_test_int @ best_beta\n",
    "\n",
    "print(\"\\nUsing best Î² on 70/30 split:\")\n",
    "print(\"RÂ² =\", r2_score(y_test, y_pred))\n",
    "print(\"MSE =\", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0ece6",
   "metadata": {},
   "source": [
    "Q2: Concept of Validation set for Multiple Linear Regression (Gradient Descent Optimization)  \n",
    "Consider the same dataset of Q1, rather than dividing the dataset into five folds, divide the dataset into training set (56%), validation set (14%), and test set (30%).  \n",
    "Consider four different values of learning rate i.e. {0.001,0.01,0.1,1}. Compute the values of regression coefficients for each value of learning rate after 1000 iterations.  \n",
    "For each set of regression coefficients, compute R2_score for validation and test set and find the best value of regression coefficients.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "822b3e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.001 -> RÂ²_val=-0.8125, RÂ²_test=-0.9914\n",
      "lr=0.01 -> RÂ²_val=0.9098, RÂ²_test=0.9147\n",
      "lr=0.1 -> RÂ²_val=0.9098, RÂ²_test=0.9148\n",
      "lr=1 -> RÂ²_val=0.9098, RÂ²_test=0.9148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 70% train+val and 30% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# temp into 80% train, 20% val (56% train, 14% val overall)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_int = add_intercept(X_train)\n",
    "X_val_int = add_intercept(X_val)\n",
    "X_test_int = add_intercept(X_test)\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, lr, iterations=1000):\n",
    "    m, n = X.shape\n",
    "    beta = np.zeros(n)\n",
    "    for _ in range(iterations):\n",
    "        preds = X @ beta\n",
    "        error = preds - y\n",
    "        grad = (1/m) * (X.T @ error)\n",
    "        beta -= lr * grad\n",
    "    return beta\n",
    "\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    beta = gradient_descent(X_train_int, y_train, lr=lr, iterations=1000)\n",
    "\n",
    "    y_val_pred = X_val_int @ beta\n",
    "    y_test_pred = X_test_int @ beta\n",
    "\n",
    "    print(f\"lr={lr} -> RÂ²_val={r2_score(y_val, y_val_pred):.4f}, RÂ²_test={r2_score(y_test, y_test_pred):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d10b9",
   "metadata": {},
   "source": [
    "Q3: Pre-processing and Multiple Linear Regression  \n",
    "Download the dataset regarding Car Price Prediction from the following link:    \n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5486f1",
   "metadata": {},
   "source": [
    "1. Load the dataset with following column names [\"symboling\", \"normalized_losses\",\n",
    "\"make\", \"fuel_type\", \"aspiration\",\"num_doors\", \"body_style\", \"drive_wheels\",\n",
    "\"engine_location\", \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\",\n",
    "\"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\", \"bore\", \"stroke\",\n",
    "\"compression_ratio\", \"horsepower\", \"peak_rpm\", \"city_mpg\", \"highway_mpg\", \"price\"]\n",
    "and replace all ? values with NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e4e0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\",\"num_doors\",\n",
    "\"body_style\", \"drive_wheels\", \"engine_location\", \"wheel_base\", \"length\", \"width\",\n",
    "\"height\", \"curb_weight\", \"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\",\n",
    "\"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\", \"city_mpg\",\n",
    "\"highway_mpg\", \"price\"]\n",
    "\n",
    "df = pd.read_csv(\"Data/imports-85.txt\", names=colnames)\n",
    "df.replace(\"?\", np.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716bf785",
   "metadata": {},
   "source": [
    "2. Replace all NaN values with central tendency imputation. Drop the rows with NaN\n",
    "values in price column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a140d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\"symboling\",\"normalized_losses\",\"wheel_base\",\"length\",\"width\",\"height\",\"curb_weight\",\n",
    "                \"engine_size\",\"bore\",\"stroke\",\"compression_ratio\",\"horsepower\",\"peak_rpm\",\n",
    "                \"city_mpg\",\"highway_mpg\",\"price\"]\n",
    "\n",
    "categorical_cols = [\"make\", \"fuel_type\", \"aspiration\",\"num_doors\", \"body_style\", \n",
    "                    \"drive_wheels\", \"engine_location\", \"engine_type\", \n",
    "                    \"num_cylinders\", \"fuel_system\"]\n",
    "\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing price\n",
    "df = df.dropna(subset=[\"price\"])\n",
    "\n",
    "# Fill NaNs: numeric cols -> mean\n",
    "for col in numeric_cols:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "# Fill NaNs: categorical cols -> mode\n",
    "for col in categorical_cols:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718de43",
   "metadata": {},
   "source": [
    "3. There are 10 columns in the dataset with non-numeric values. Convert these values to  \n",
    "numeric values using following scheme:  \n",
    "(i) For â€œnum_doorsâ€ and â€œnum_cylindersâ€: convert words (number names) to figures for e.g., two to 2    \n",
    "(ii) For \"body_style\", \"drive_wheels\": use dummy encoding scheme  \n",
    "(iii) For â€œmakeâ€, â€œaspirationâ€, â€œengine_locationâ€,fuel_type: use label encoding scheme    \n",
    "(iv) For fuel_system: replace values containing string pfi to 1 else all values to 0.  \n",
    "(v) For engine_type: replace values containing string ohc to 1 else all values to 0.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96181237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert word numbers to digits\n",
    "mapping = {\"two\":2, \"four\":4, \"six\":6, \"eight\":8, \"twelve\":12}\n",
    "df[\"num_doors\"] = df[\"num_doors\"].map(mapping).fillna(4).astype(int)\n",
    "df[\"num_cylinders\"] = df[\"num_cylinders\"].map(mapping).fillna(4).astype(int)\n",
    "\n",
    "# Dummy encoding\n",
    "df = pd.get_dummies(df, columns=[\"body_style\",\"drive_wheels\"], drop_first=True)\n",
    "\n",
    "# Label encoding\n",
    "for col in [\"make\",\"aspiration\",\"engine_location\",\"fuel_type\"]:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Fuel system -> binary\n",
    "df[\"fuel_system\"] = df[\"fuel_system\"].str.contains(\"pfi\").astype(int)\n",
    "\n",
    "# Engine type -> binary\n",
    "df[\"engine_type\"] = df[\"engine_type\"].str.contains(\"ohc\").astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d36b96",
   "metadata": {},
   "source": [
    "4. Divide the dataset into input features (all columns except price) and output variable (price).  \n",
    "Scale all input features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cf7d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = df.drop(\"price\", axis=1).values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb01137",
   "metadata": {},
   "source": [
    "5. Train a linear regressor on 70% of data (using inbuilt linear regression function of Python) and test its performance on remaining 30% of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddf084e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RÂ²: 0.8720694422097159\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print(\"Linear Regression RÂ²:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50598256",
   "metadata": {},
   "source": [
    "6. Reduce the dimensionality of the feature set using inbuilt PCA decomposition and then\n",
    "again train a linear regressor on 70% of reduced data (using inbuilt linear regression\n",
    "function of Python). Does it lead to any performance improvement on test set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51f20861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After PCA RÂ²: 0.8602756046273354\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.95)  # 95% variance\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_reduced, y, test_size=0.3, random_state=42)\n",
    "\n",
    "reg2 = LinearRegression()\n",
    "reg2.fit(Xr_train, yr_train)\n",
    "yr_pred = reg2.predict(Xr_test)\n",
    "\n",
    "print(\"After PCA RÂ²:\", r2_score(yr_test, yr_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328843ef",
   "metadata": {},
   "source": [
    "PCA did not lead to performance improvement, since the RÂ² score decreased slightly after dimensionality reduction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
